{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7845dafd",
   "metadata": {},
   "source": [
    "- created a repository github. copied http url\n",
    "- opened terminal 'git clone http..'\n",
    "- edited a file.\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6c04fa",
   "metadata": {},
   "source": [
    "- git status \n",
    "- git add .\n",
    "- git commit -m 'msg'\n",
    "- git push -u origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138e2002",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5196f73c",
   "metadata": {},
   "source": [
    "# 🧮 Forward Propagation: Feedforward Neural Network\n",
    "\n",
    "## 🧠 Network Architecture:\n",
    "- **Input:** 2 features: $x = [1, 2]$\n",
    "- **Hidden Layer 1:** 2 neurons (ReLU)\n",
    "- **Hidden Layer 2:** 2 neurons (ReLU)\n",
    "- **Output Layer:** 1 neuron (Sigmoid)\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 Parameters:\n",
    "\n",
    "### Hidden Layer 1\n",
    "$$\n",
    "W^{[1]} = \\begin{bmatrix} 0.1 & 0.2 \\\\ 0.3 & 0.4 \\end{bmatrix}, \\quad\n",
    "b^{[1]} = \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Hidden Layer 2\n",
    "$$\n",
    "W^{[2]} = \\begin{bmatrix} 0.5 & 0.6 \\\\ 0.7 & 0.8 \\end{bmatrix}, \\quad\n",
    "b^{[2]} = \\begin{bmatrix} 0.3 \\\\ 0.4 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Output Layer\n",
    "$$\n",
    "W^{[3]} = \\begin{bmatrix} 0.9 & 1.0 \\end{bmatrix}, \\quad\n",
    "b^{[3]} = \\begin{bmatrix} 0.5 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ➕ Step-by-step Calculations\n",
    "\n",
    "### 📥 Input\n",
    "$$\n",
    "x = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Layer 1: Linear + ReLU\n",
    "$$\n",
    "z^{[1]} = W^{[1]}x + b^{[1]} = \\begin{bmatrix} 0.1 & 0.2 \\\\ 0.3 & 0.4 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix}\n",
    "= \\begin{bmatrix} 0.6 \\\\ 1.3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**ReLU Activation:**\n",
    "$$\n",
    "a^{[1]} = \\max(0, z^{[1]}) = \\begin{bmatrix} 0.6 \\\\ 1.3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Layer 2: Linear + ReLU\n",
    "$$\n",
    "z^{[2]} = W^{[2]}a^{[1]} + b^{[2]} = \\begin{bmatrix} 0.5 & 0.6 \\\\ 0.7 & 0.8 \\end{bmatrix} \\begin{bmatrix} 0.6 \\\\ 1.3 \\end{bmatrix} + \\begin{bmatrix} 0.3 \\\\ 0.4 \\end{bmatrix}\n",
    "= \\begin{bmatrix} 1.38 \\\\ 1.86 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**ReLU Activation:**\n",
    "$$\n",
    "a^{[2]} = \\max(0, z^{[2]}) = \\begin{bmatrix} 1.38 \\\\ 1.86 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Output Layer: Linear + Sigmoid\n",
    "$$\n",
    "z^{[3]} = W^{[3]}a^{[2]} + b^{[3]} = \\begin{bmatrix} 0.9 & 1.0 \\end{bmatrix} \\begin{bmatrix} 1.38 \\\\ 1.86 \\end{bmatrix} + 0.5 = 3.602\n",
    "$$\n",
    "\n",
    "**Sigmoid Activation:**\n",
    "$$\n",
    "\\hat{y} = \\sigma(z^{[3]}) = \\frac{1}{1 + e^{-3.602}} \\approx 0.9736\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Final Output:\n",
    "$$\n",
    "\\hat{y} = 0.9736\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbb8d40",
   "metadata": {},
   "source": [
    "# 🔁 Common Activation Functions in Neural Networks\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Sigmoid (Logistic)\n",
    "\n",
    "**Equation:**\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "**Range:**  \n",
    "$ (0, 1) $\n",
    "\n",
    "**Use When:**\n",
    "- Binary classification (used in the output layer)\n",
    "- Models requiring probabilities\n",
    "\n",
    "**Pros:** Smooth gradient, probabilistic output  \n",
    "**Cons:** Vanishing gradient for large inputs\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Tanh (Hyperbolic Tangent)\n",
    "\n",
    "**Equation:**\n",
    "$$\n",
    "\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$\n",
    "\n",
    "**Range:**  \n",
    "$ (-1, 1) $\n",
    "\n",
    "**Use When:**\n",
    "- Hidden layers when zero-centered output is preferred\n",
    "\n",
    "**Pros:** Stronger gradients than sigmoid  \n",
    "**Cons:** Still suffers from vanishing gradients for large values\n",
    "\n",
    "---\n",
    "\n",
    "## 3. ReLU (Rectified Linear Unit)\n",
    "\n",
    "**Equation:**\n",
    "$$\n",
    "f(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "**Range:**  \n",
    "$ [0, \\infty) $\n",
    "\n",
    "**Use When:**\n",
    "- Default for hidden layers\n",
    "- Most popular due to simplicity and performance\n",
    "\n",
    "**Pros:** Efficient, less vanishing gradient  \n",
    "**Cons:** Can die (output zero) for negative inputs (\"dying ReLU\")\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Leaky ReLU\n",
    "\n",
    "**Equation:**\n",
    "$$\n",
    "f(x) = \\begin{cases}\n",
    "x & \\text{if } x > 0 \\\\\n",
    "\\alpha x & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "(typically $ \\alpha = 0.01 $)\n",
    "\n",
    "**Range:**  \n",
    "$ (-\\infty, \\infty) $\n",
    "\n",
    "**Use When:**\n",
    "- Fix for dying ReLU problem\n",
    "\n",
    "**Pros:** Allows small gradient when $x < 0$  \n",
    "**Cons:** Small slope may still cause slow learning\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Softmax\n",
    "\n",
    "**Equation (for multi-class classification):**\n",
    "$$\n",
    "\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n",
    "$$\n",
    "\n",
    "**Range:**  \n",
    "$ (0, 1) $, and all outputs sum to 1\n",
    "\n",
    "**Use When:**\n",
    "- Multi-class classification (used in the output layer)\n",
    "\n",
    "**Pros:** Gives probability distribution over classes  \n",
    "**Cons:** Not used in hidden layers\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Table\n",
    "\n",
    "| Activation     | Range             | Use Case                     |\n",
    "|----------------|------------------|------------------------------|\n",
    "| Sigmoid        | (0, 1)           | Binary classification output |\n",
    "| Tanh           | (-1, 1)          | Hidden layers (zero-centered)|\n",
    "| ReLU           | [0, ∞)           | Most hidden layers           |\n",
    "| Leaky ReLU     | (-∞, ∞)          | Hidden layers (ReLU fix)     |\n",
    "| Softmax        | (0, 1), sum=1    | Output of multi-class model  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d81357",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d66a989c",
   "metadata": {},
   "source": [
    "# 🧱 Common Keras Layers and Their Usage\n",
    "\n",
    "Keras provides a wide range of layers to build neural network architectures. Below are some of the most commonly used layers:\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 Dense Layer (Fully Connected Layer)\n",
    "\n",
    "- Connects every neuron in the current layer to every neuron in the next layer.\n",
    "- Commonly used in feedforward neural networks and as the output layer.\n",
    "- Supports activation functions like ReLU, Sigmoid, Softmax, etc.\n",
    "\n",
    "**Use Case:** Classification, regression, MLPs\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 Activation Layer\n",
    "\n",
    "- Applies an activation function to the inputs.\n",
    "- Can be used standalone or with layers like Dense and Conv2D.\n",
    "\n",
    "**Use Case:** Adds non-linearity (e.g., ReLU, Sigmoid, Tanh, Softmax)\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 Input Layer\n",
    "\n",
    "- Defines the shape and data type of the input.\n",
    "- Required when using the Functional API.\n",
    "\n",
    "**Use Case:** Entry point for models built using Functional API\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 Dropout Layer\n",
    "\n",
    "- Randomly sets a fraction of input units to zero during training.\n",
    "- Helps prevent overfitting by promoting generalization.\n",
    "\n",
    "**Use Case:** Between layers in deep networks to regularize\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 Flatten Layer\n",
    "\n",
    "- Converts multi-dimensional input into a 1D vector.\n",
    "- Typically used to move from convolutional to dense layers.\n",
    "\n",
    "**Use Case:** In CNNs before feeding into fully connected layers\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 Conv2D Layer\n",
    "\n",
    "- Performs 2D convolution on input images.\n",
    "- Extracts spatial features like edges, textures, and patterns.\n",
    "\n",
    "**Use Case:** Image classification, object detection\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 MaxPooling2D Layer\n",
    "\n",
    "- Reduces the spatial size of feature maps.\n",
    "- Helps in downsampling and reducing computation.\n",
    "\n",
    "**Use Case:** After Conv2D to reduce spatial dimensions\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 BatchNormalization Layer\n",
    "\n",
    "- Normalizes the inputs across the batch.\n",
    "- Accelerates training and improves performance.\n",
    "\n",
    "**Use Case:** Between layers to stabilize and accelerate training\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 LSTM Layer\n",
    "\n",
    "- A type of recurrent neural network (RNN) that handles long-term dependencies.\n",
    "- Suitable for sequential and time-series data.\n",
    "\n",
    "**Use Case:** Text processing, speech recognition, time series prediction\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Summary Table\n",
    "\n",
    "| Layer               | Description                                | Common Use Case                     |\n",
    "|--------------------|--------------------------------------------|-------------------------------------|\n",
    "| Dense              | Fully connected layer                      | Classification, regression          |\n",
    "| Activation         | Applies non-linearity                     | All types of models                 |\n",
    "| Input              | Specifies input shape                      | Functional API                      |\n",
    "| Dropout            | Randomly disables neurons                 | Preventing overfitting              |\n",
    "| Flatten            | Flattens input                            | CNN to Dense transition             |\n",
    "| Conv2D             | 2D convolution for images                 | Image processing, CNNs              |\n",
    "| MaxPooling2D       | Downsamples feature maps                  | Reducing spatial dimensions         |\n",
    "| BatchNormalization | Normalizes layer input                    | Speeds up and stabilizes training   |\n",
    "| LSTM               | Memory for sequences                      | Time series, text, sequences        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3345f450",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
